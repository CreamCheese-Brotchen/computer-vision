{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Short Discription:}$  \n",
    "This Jupyter notebook primarily focuses on the implementation of Unet model and its application from scratch on the Image Segmentation problem. Training was performed using the CWFID dataset and \n",
    "\n",
    "In the 'Image Classification with U-Net' folder, Unet model was applied on image segmentation with the CWFID dataset. Given the limited size of the CWFID dataset, this experiment is tried to enhance performance through Transfer Learning. To achieve this, I defined a classification model that is identical to the first half of the Unet model. This model was trained on the 'Deep Weeds' dataset using cross-entropy loss.\n",
    "\n",
    "Following that, I fine-tuned the U-Net model using the CWFID dataset. During this fine-tuning process, all layers were kept frozen with pretrained weights, and only the second half of the model were trained, finally fine-tuned the entire model on the CWFID dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\python\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(1)\n",
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "#from google.colab import files\n",
    "import tensorflow_datasets as tfds\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "There are two dataset used in this experiment, \"Deep Weeds\" and \"CWFID\", where the \"Deep Weeds\" is used for pretraining the custome classification model, as these two datasets are similar (Deep weeds is available in tfds-nightly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_deepweeds(reduce_num=None):\n",
    "  train_x = []\n",
    "  train_y = []\n",
    "  ds = tfds.load('deep_weeds', split='train', as_supervised=True)\n",
    "  for image,label in tfds.as_numpy(ds):\n",
    "    train_x.append(np.array(Image.fromarray(image).resize((224,224))))\n",
    "    train_y.append(label)\n",
    "  if reduce_num:\n",
    "    train_x = train_x[:reduce_num]\n",
    "    train_y = train_y[:reduce_num]\n",
    "  train_x = np.array(train_x)/255.0\n",
    "  train_y = tf.keras.utils.to_categorical(np.array(train_y))\n",
    "  return train_x, train_y\n",
    "\n",
    "\n",
    "def load_segmentation_data():\n",
    "  data = np.load('segmentation_data.npz')\n",
    "  train_x = data['train_x']\n",
    "  train_y = data['train_y']\n",
    "  test_x = data['test_x']\n",
    "  test_y = data['test_y']\n",
    "  return train_x, train_y, test_x, test_y\n",
    "\n",
    "train_x2, train_y2 = load_deepweeds(reduce_num=1000)\n",
    "train_x, train_y, test_x, test_y = load_segmentation_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models architecture and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-entropy loss at individual spatial positions, averaged over positions\n",
    "@tf.function\n",
    "def mycrossentropy(y_true, y_pred):\n",
    "    losses = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred, axis=3) # compute crossentropy loss from logits, for each spatial position\n",
    "    return tf.math.reduce_mean(losses,axis=[1,2]) # average out the two axes corresponding to spatial position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_model():\n",
    "  inputs = tf.keras.layers.Input((224,224,3))\n",
    "  x = inputs\n",
    "  x224 = tf.keras.layers.Conv2D(16,3,padding='same',activation='relu',strides=1,name='L224')(inputs)\n",
    "  x112 = tf.keras.layers.Conv2D(32,3,padding='same',activation='relu',strides=2,name='L112a')(x224)\n",
    "  x112 = tf.keras.layers.Conv2D(32,3,padding='same',activation='relu',strides=1,name='L112')(x112)\n",
    "  x56 = tf.keras.layers.Conv2D(64,3,padding='same',activation='relu',strides=2,name='L56a')(x112)\n",
    "  x56 = tf.keras.layers.Conv2D(64,3,padding='same',activation='relu',strides=1,name='L56')(x56)\n",
    "  x28 = tf.keras.layers.Conv2D(128,3,padding='same',activation='relu',strides=2,name='L28a')(x56)\n",
    "  x28 = tf.keras.layers.Conv2D(128,3,padding='same',activation='relu',strides=1,name='L28')(x28)\n",
    "  x14 = tf.keras.layers.Conv2D(256,3,padding='same',activation='relu',strides=2,name='L14a')(x28)\n",
    "  x14 = tf.keras.layers.Conv2D(256,3,padding='same',activation='relu',strides=1,name='L14')(x14)\n",
    "  x = tf.keras.layers.GlobalAveragePooling2D()(x14)\n",
    "  outputs = tf.keras.layers.Dense(9,activation='softmax')(x)\n",
    "  model = tf.keras.Model(inputs=inputs,outputs=outputs)\n",
    "  model.summary()\n",
    "  return model\n",
    "\n",
    "def unet_model():\n",
    "  inputs = tf.keras.layers.Input((224,224,3))\n",
    "  x = inputs\n",
    "  x224 = tf.keras.layers.Conv2D(16,3,padding='same',activation='relu',strides=1,name='L224')(inputs)\n",
    "  x112 = tf.keras.layers.Conv2D(32,3,padding='same',activation='relu',strides=2,name='L112a')(x224)\n",
    "  x112 = tf.keras.layers.Conv2D(32,3,padding='same',activation='relu',strides=1,name='L112')(x112)\n",
    "  x56 = tf.keras.layers.Conv2D(64,3,padding='same',activation='relu',strides=2,name='L56a')(x112)\n",
    "  x56 = tf.keras.layers.Conv2D(64,3,padding='same',activation='relu',strides=1,name='L56')(x56)\n",
    "  x28 = tf.keras.layers.Conv2D(128,3,padding='same',activation='relu',strides=2,name='L28a')(x56)\n",
    "  x28 = tf.keras.layers.Conv2D(128,3,padding='same',activation='relu',strides=1,name='L28')(x28)\n",
    "  x14 = tf.keras.layers.Conv2D(256,3,padding='same',activation='relu',strides=2,name='L14a')(x28)\n",
    "  x14 = tf.keras.layers.Conv2D(256,3,padding='same',activation='relu',strides=1,name='L14')(x14)\n",
    "  x = tf.keras.layers.Conv2DTranspose(128,3,strides=2,padding='same',activation='relu')(x14)\n",
    "  x = tf.keras.layers.Conv2D(128,3,padding='same',activation='relu',strides=1)(x)\n",
    "  x = tf.keras.layers.Concatenate()([x,x28])\n",
    "  x = tf.keras.layers.Conv2DTranspose(64,3,strides=2,padding='same',activation='relu')(x)\n",
    "  x = tf.keras.layers.Conv2D(64,3,padding='same',activation='relu',strides=1)(x)\n",
    "  x = tf.keras.layers.Concatenate()([x,x56])\n",
    "  x = tf.keras.layers.Conv2DTranspose(32,3,strides=2,padding='same',activation='relu')(x)\n",
    "  x = tf.keras.layers.Conv2D(32,3,padding='same',activation='relu',strides=1)(x)\n",
    "  x = tf.keras.layers.Concatenate()([x,x112])\n",
    "  x = tf.keras.layers.Conv2DTranspose(16,3,strides=2,padding='same',activation='relu')(x)\n",
    "  x = tf.keras.layers.Conv2D(16,3,padding='same',activation='relu',strides=1)(x)\n",
    "  x = tf.keras.layers.Concatenate()([x,x224])\n",
    "  outputs = tf.keras.layers.Conv2D(3,1,padding='same')(x)\n",
    "  unet = tf.keras.Model(inputs=inputs,outputs=outputs)\n",
    "  unet.summary()\n",
    "  return unet\n",
    "\n",
    "\n",
    "\n",
    "def unet_model_transfer(model):\n",
    "  inputs = model.input\n",
    "  x = model.get_layer('L14').output\n",
    "\n",
    "  x = tf.keras.layers.Conv2DTranspose(128, 3, strides=(2,2), padding='same', activation='relu')(x)\n",
    "  x = tf.keras.layers.Conv2D(128,3,padding='same',activation='relu',strides=1)(x)\n",
    "  x = tf.keras.layers.Concatenate()([x,model.get_layer('L28').output])\n",
    "\n",
    "  x = tf.keras.layers.Conv2DTranspose(64, 3, strides=(2,2), padding='same', activation='relu')(x)\n",
    "  x = tf.keras.layers.Conv2D(64,3,padding='same',activation='relu',strides=1)(x)\n",
    "  x = tf.keras.layers.Concatenate()([x,model.get_layer('L56').output])\n",
    "\n",
    "  x = tf.keras.layers.Conv2DTranspose(32, 3, strides=(2,2), padding='same', activation='relu')(x)\n",
    "  x = tf.keras.layers.Conv2D(32,3,padding='same',activation='relu',strides=1)(x)\n",
    "  x = tf.keras.layers.Concatenate()([x,model.get_layer('L112').output])\n",
    "\n",
    "  x = tf.keras.layers.Conv2DTranspose(16, 3, strides=(2,2), padding='same', activation='relu')(x)\n",
    "  x = tf.keras.layers.Conv2D(16,3,padding='same',activation='relu',strides=1)(x)\n",
    "  x = tf.keras.layers.Concatenate()([x,model.get_layer('L224').output])\n",
    "\n",
    "  outputs = tf.keras.layers.Conv2D(3, 1, padding='same')(x)\n",
    "\n",
    "  unet = tf.keras.Model(inputs=model.inputs,outputs=outputs)\n",
    "  unet.summary()\n",
    "  return unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions_as_images(predictions,test_y):\n",
    "    pred_y = np.zeros(predictions.shape)\n",
    "    # convert class scores to one-hot encoded predictions\n",
    "    for i in range(0,test_x.shape[0]):\n",
    "        for x in range(0,test_x.shape[1]):\n",
    "            for y in range(0,test_x.shape[2]):\n",
    "                pred_y[i,x,y,np.argmax(predictions[i,x,y,:])]=1\n",
    "    # plot the three classes as RGB image \n",
    "    for i in range(0,test_x.shape[0]):\n",
    "        Image.fromarray((test_y[i,:,:,:]*255).astype('uint8')).save('%i_true.png' % i)\n",
    "        Image.fromarray((pred_y[i,:,:,:]*255).astype('uint8')).save('%i_pred.png' % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training \n",
    "\n",
    "Due to the computational limitation, the dataset size and the training epochs were reduced, which led to not ideal performance. Additionally, the predition images and original images are saved in the folder \"predition images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " L224 (Conv2D)                  (None, 224, 224, 16  448         ['input_3[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " L112a (Conv2D)                 (None, 112, 112, 32  4640        ['L224[0][0]']                   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " L112 (Conv2D)                  (None, 112, 112, 32  9248        ['L112a[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " L56a (Conv2D)                  (None, 56, 56, 64)   18496       ['L112[0][0]']                   \n",
      "                                                                                                  \n",
      " L56 (Conv2D)                   (None, 56, 56, 64)   36928       ['L56a[0][0]']                   \n",
      "                                                                                                  \n",
      " L28a (Conv2D)                  (None, 28, 28, 128)  73856       ['L56[0][0]']                    \n",
      "                                                                                                  \n",
      " L28 (Conv2D)                   (None, 28, 28, 128)  147584      ['L28a[0][0]']                   \n",
      "                                                                                                  \n",
      " L14a (Conv2D)                  (None, 14, 14, 256)  295168      ['L28[0][0]']                    \n",
      "                                                                                                  \n",
      " L14 (Conv2D)                   (None, 14, 14, 256)  590080      ['L14a[0][0]']                   \n",
      "                                                                                                  \n",
      " conv2d_transpose_8 (Conv2DTran  (None, 28, 28, 128)  295040     ['L14[0][0]']                    \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 28, 28, 128)  147584      ['conv2d_transpose_8[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 28, 28, 256)  0           ['conv2d_10[0][0]',              \n",
      "                                                                  'L28[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_transpose_9 (Conv2DTran  (None, 56, 56, 64)  147520      ['concatenate_8[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 56, 56, 64)   36928       ['conv2d_transpose_9[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 56, 56, 128)  0           ['conv2d_11[0][0]',              \n",
      "                                                                  'L56[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_transpose_10 (Conv2DTra  (None, 112, 112, 32  36896      ['concatenate_9[0][0]']          \n",
      " nspose)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 112, 112, 32  9248        ['conv2d_transpose_10[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 112, 112, 64  0           ['conv2d_12[0][0]',              \n",
      "                                )                                 'L112[0][0]']                   \n",
      "                                                                                                  \n",
      " conv2d_transpose_11 (Conv2DTra  (None, 224, 224, 16  9232       ['concatenate_10[0][0]']         \n",
      " nspose)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 224, 224, 16  2320        ['conv2d_transpose_11[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 224, 224, 32  0           ['conv2d_13[0][0]',              \n",
      "                                )                                 'L224[0][0]']                   \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 224, 224, 3)  99          ['concatenate_11[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,861,315\n",
      "Trainable params: 1,861,315\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "4/4 [==============================] - 9s 2s/step - loss: 1.0542 - accuracy: 0.1134 - val_loss: 1.0100 - val_accuracy: 0.6498\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.9460 - accuracy: 0.8520 - val_loss: 0.6824 - val_accuracy: 0.9244\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.6840 - accuracy: 0.9252 - val_loss: 0.4099 - val_accuracy: 0.9259\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.5273 - accuracy: 0.9256 - val_loss: 0.5616 - val_accuracy: 0.9259\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.4883 - accuracy: 0.9256 - val_loss: 0.3512 - val_accuracy: 0.9259\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.3751 - accuracy: 0.9256 - val_loss: 0.3694 - val_accuracy: 0.9259\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.3438 - accuracy: 0.9256 - val_loss: 0.3142 - val_accuracy: 0.9259\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.3110 - accuracy: 0.9256 - val_loss: 0.2941 - val_accuracy: 0.9259\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.2817 - accuracy: 0.9256 - val_loss: 0.2652 - val_accuracy: 0.9259\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.2518 - accuracy: 0.9256 - val_loss: 0.2298 - val_accuracy: 0.9259\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.2213 - accuracy: 0.9256 - val_loss: 0.1990 - val_accuracy: 0.9259\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.1925 - accuracy: 0.9256 - val_loss: 0.1765 - val_accuracy: 0.9259\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.1746 - accuracy: 0.9256 - val_loss: 0.1610 - val_accuracy: 0.9260\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.1599 - accuracy: 0.9258 - val_loss: 0.1508 - val_accuracy: 0.9265\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.1503 - accuracy: 0.9267 - val_loss: 0.1438 - val_accuracy: 0.9274\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.1442 - accuracy: 0.9276 - val_loss: 0.1385 - val_accuracy: 0.9292\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.1384 - accuracy: 0.9297 - val_loss: 0.1329 - val_accuracy: 0.9306\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.1339 - accuracy: 0.9326 - val_loss: 0.1311 - val_accuracy: 0.9332\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.1307 - accuracy: 0.9365 - val_loss: 0.1271 - val_accuracy: 0.9385\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.1279 - accuracy: 0.9412 - val_loss: 0.1246 - val_accuracy: 0.9439\n",
      "1/1 [==============================] - 1s 929ms/step\n"
     ]
    }
   ],
   "source": [
    "running_epochs = 20 # 500\n",
    "\n",
    "unet = unet_model()\n",
    "unet.compile(loss=mycrossentropy,optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),metrics=['accuracy']) \n",
    "unet.fit(train_x, train_y, batch_size = 10, epochs = running_epochs, validation_data=(test_x, test_y))\n",
    "\n",
    "predictions = unet.predict(test_x)\n",
    "save_predictions_as_images(predictions,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " L224 (Conv2D)               (None, 224, 224, 16)      448       \n",
      "                                                                 \n",
      " L112a (Conv2D)              (None, 112, 112, 32)      4640      \n",
      "                                                                 \n",
      " L112 (Conv2D)               (None, 112, 112, 32)      9248      \n",
      "                                                                 \n",
      " L56a (Conv2D)               (None, 56, 56, 64)        18496     \n",
      "                                                                 \n",
      " L56 (Conv2D)                (None, 56, 56, 64)        36928     \n",
      "                                                                 \n",
      " L28a (Conv2D)               (None, 28, 28, 128)       73856     \n",
      "                                                                 \n",
      " L28 (Conv2D)                (None, 28, 28, 128)       147584    \n",
      "                                                                 \n",
      " L14a (Conv2D)               (None, 14, 14, 256)       295168    \n",
      "                                                                 \n",
      " L14 (Conv2D)                (None, 14, 14, 256)       590080    \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 256)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 9)                 2313      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,178,761\n",
      "Trainable params: 1,178,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "32/32 [==============================] - 49s 2s/step - loss: 1.8099 - accuracy: 0.5170\n",
      "Epoch 2/25\n",
      "32/32 [==============================] - 48s 1s/step - loss: 1.6600 - accuracy: 0.5310\n",
      "Epoch 3/25\n",
      "32/32 [==============================] - 47s 1s/step - loss: 1.6102 - accuracy: 0.5310\n",
      "Epoch 4/25\n",
      "32/32 [==============================] - 48s 1s/step - loss: 1.6964 - accuracy: 0.5250\n",
      "Epoch 5/25\n",
      "32/32 [==============================] - 47s 1s/step - loss: 1.5075 - accuracy: 0.5340\n",
      "Epoch 6/25\n",
      "32/32 [==============================] - 48s 1s/step - loss: 1.4911 - accuracy: 0.5350\n",
      "Epoch 7/25\n",
      "32/32 [==============================] - 47s 1s/step - loss: 1.4907 - accuracy: 0.5270\n",
      "Epoch 8/25\n",
      "32/32 [==============================] - 47s 1s/step - loss: 1.5304 - accuracy: 0.5330\n",
      "Epoch 9/25\n",
      "32/32 [==============================] - 47s 1s/step - loss: 1.5257 - accuracy: 0.5220\n",
      "Epoch 10/25\n",
      "32/32 [==============================] - 47s 1s/step - loss: 1.4778 - accuracy: 0.5390\n",
      "Epoch 11/25\n",
      "32/32 [==============================] - 47s 1s/step - loss: 1.4835 - accuracy: 0.5320\n",
      "Epoch 12/25\n",
      "32/32 [==============================] - 46s 1s/step - loss: 1.4678 - accuracy: 0.5340\n",
      "Epoch 13/25\n",
      "32/32 [==============================] - 47s 1s/step - loss: 1.4478 - accuracy: 0.5360\n",
      "Epoch 14/25\n",
      "32/32 [==============================] - 47s 1s/step - loss: 1.4511 - accuracy: 0.5350\n",
      "Epoch 15/25\n",
      "32/32 [==============================] - 46s 1s/step - loss: 1.4579 - accuracy: 0.5260\n",
      "Epoch 16/25\n",
      "32/32 [==============================] - 47s 1s/step - loss: 1.4418 - accuracy: 0.5330\n",
      "Epoch 17/25\n",
      "32/32 [==============================] - 46s 1s/step - loss: 1.4469 - accuracy: 0.5300\n",
      "Epoch 18/25\n",
      "32/32 [==============================] - 47s 1s/step - loss: 1.4293 - accuracy: 0.5440\n",
      "Epoch 19/25\n",
      "32/32 [==============================] - 46s 1s/step - loss: 1.4313 - accuracy: 0.5480\n",
      "Epoch 20/25\n",
      "32/32 [==============================] - 47s 1s/step - loss: 1.4733 - accuracy: 0.5340\n",
      "Epoch 21/25\n",
      "32/32 [==============================] - 46s 1s/step - loss: 1.4361 - accuracy: 0.5380\n",
      "Epoch 22/25\n",
      "32/32 [==============================] - 47s 1s/step - loss: 1.4263 - accuracy: 0.5350\n",
      "Epoch 23/25\n",
      "32/32 [==============================] - 47s 1s/step - loss: 1.4350 - accuracy: 0.5390\n",
      "Epoch 24/25\n",
      "32/32 [==============================] - 46s 1s/step - loss: 1.3897 - accuracy: 0.5480\n",
      "Epoch 25/25\n",
      "32/32 [==============================] - 46s 1s/step - loss: 1.4065 - accuracy: 0.5460\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " L224 (Conv2D)                  (None, 224, 224, 16  448         ['input_5[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " L112a (Conv2D)                 (None, 112, 112, 32  4640        ['L224[0][0]']                   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " L112 (Conv2D)                  (None, 112, 112, 32  9248        ['L112a[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " L56a (Conv2D)                  (None, 56, 56, 64)   18496       ['L112[0][0]']                   \n",
      "                                                                                                  \n",
      " L56 (Conv2D)                   (None, 56, 56, 64)   36928       ['L56a[0][0]']                   \n",
      "                                                                                                  \n",
      " L28a (Conv2D)                  (None, 28, 28, 128)  73856       ['L56[0][0]']                    \n",
      "                                                                                                  \n",
      " L28 (Conv2D)                   (None, 28, 28, 128)  147584      ['L28a[0][0]']                   \n",
      "                                                                                                  \n",
      " L14a (Conv2D)                  (None, 14, 14, 256)  295168      ['L28[0][0]']                    \n",
      "                                                                                                  \n",
      " L14 (Conv2D)                   (None, 14, 14, 256)  590080      ['L14a[0][0]']                   \n",
      "                                                                                                  \n",
      " conv2d_transpose_16 (Conv2DTra  (None, 28, 28, 128)  295040     ['L14[0][0]']                    \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 28, 28, 128)  147584      ['conv2d_transpose_16[0][0]']    \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenate)   (None, 28, 28, 256)  0           ['conv2d_20[0][0]',              \n",
      "                                                                  'L28[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_transpose_17 (Conv2DTra  (None, 56, 56, 64)  147520      ['concatenate_16[0][0]']         \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 56, 56, 64)   36928       ['conv2d_transpose_17[0][0]']    \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenate)   (None, 56, 56, 128)  0           ['conv2d_21[0][0]',              \n",
      "                                                                  'L56[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_transpose_18 (Conv2DTra  (None, 112, 112, 32  36896      ['concatenate_17[0][0]']         \n",
      " nspose)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 112, 112, 32  9248        ['conv2d_transpose_18[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_18 (Concatenate)   (None, 112, 112, 64  0           ['conv2d_22[0][0]',              \n",
      "                                )                                 'L112[0][0]']                   \n",
      "                                                                                                  \n",
      " conv2d_transpose_19 (Conv2DTra  (None, 224, 224, 16  9232       ['concatenate_18[0][0]']         \n",
      " nspose)                        )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 224, 224, 16  2320        ['conv2d_transpose_19[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_19 (Concatenate)   (None, 224, 224, 32  0           ['conv2d_23[0][0]',              \n",
      "                                )                                 'L224[0][0]']                   \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 224, 224, 3)  99          ['concatenate_19[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,861,315\n",
      "Trainable params: 1,861,315\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Setting layer L224 to not trainable\n",
      "Setting layer L112a to not trainable\n",
      "Setting layer L112 to not trainable\n",
      "Setting layer L56a to not trainable\n",
      "Setting layer L56 to not trainable\n",
      "Setting layer L28a to not trainable\n",
      "Setting layer L28 to not trainable\n",
      "Setting layer L14a to not trainable\n",
      "Setting layer L14 to not trainable\n",
      "Epoch 1/20\n",
      "4/4 [==============================] - 6s 1s/step - loss: 1.1998 - accuracy: 0.6504 - val_loss: 1.0168 - val_accuracy: 0.9191\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.9936 - accuracy: 0.9231 - val_loss: 0.9470 - val_accuracy: 0.9258\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.9125 - accuracy: 0.9256 - val_loss: 0.8477 - val_accuracy: 0.9259\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.7978 - accuracy: 0.9256 - val_loss: 0.7013 - val_accuracy: 0.9259\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.5744 - accuracy: 0.9256 - val_loss: 0.3570 - val_accuracy: 0.9259\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.3533 - accuracy: 0.9256 - val_loss: 0.3221 - val_accuracy: 0.9259\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.3271 - accuracy: 0.9256 - val_loss: 0.2961 - val_accuracy: 0.9259\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.3066 - accuracy: 0.9256 - val_loss: 0.2933 - val_accuracy: 0.9259\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.2915 - accuracy: 0.9256 - val_loss: 0.2894 - val_accuracy: 0.9259\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.2896 - accuracy: 0.9256 - val_loss: 0.2828 - val_accuracy: 0.9259\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.2838 - accuracy: 0.9256 - val_loss: 0.2760 - val_accuracy: 0.9259\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.2779 - accuracy: 0.9256 - val_loss: 0.2684 - val_accuracy: 0.9259\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.2737 - accuracy: 0.9256 - val_loss: 0.2657 - val_accuracy: 0.9259\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.2703 - accuracy: 0.9256 - val_loss: 0.2628 - val_accuracy: 0.9259\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.2666 - accuracy: 0.9256 - val_loss: 0.2606 - val_accuracy: 0.9259\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.2637 - accuracy: 0.9256 - val_loss: 0.2581 - val_accuracy: 0.9259\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.2600 - accuracy: 0.9256 - val_loss: 0.2540 - val_accuracy: 0.9259\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.2560 - accuracy: 0.9256 - val_loss: 0.2519 - val_accuracy: 0.9259\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.2529 - accuracy: 0.9256 - val_loss: 0.2466 - val_accuracy: 0.9259\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.2491 - accuracy: 0.9256 - val_loss: 0.2446 - val_accuracy: 0.9259\n",
      "Setting layer input_5 to trainable\n",
      "Setting layer L224 to trainable\n",
      "Setting layer L112a to trainable\n",
      "Setting layer L112 to trainable\n",
      "Setting layer L56a to trainable\n",
      "Setting layer L56 to trainable\n",
      "Setting layer L28a to trainable\n",
      "Setting layer L28 to trainable\n",
      "Setting layer L14a to trainable\n",
      "Setting layer L14 to trainable\n",
      "Setting layer conv2d_transpose_16 to trainable\n",
      "Setting layer conv2d_20 to trainable\n",
      "Setting layer concatenate_16 to trainable\n",
      "Setting layer conv2d_transpose_17 to trainable\n",
      "Setting layer conv2d_21 to trainable\n",
      "Setting layer concatenate_17 to trainable\n",
      "Setting layer conv2d_transpose_18 to trainable\n",
      "Setting layer conv2d_22 to trainable\n",
      "Setting layer concatenate_18 to trainable\n",
      "Setting layer conv2d_transpose_19 to trainable\n",
      "Setting layer conv2d_23 to trainable\n",
      "Setting layer concatenate_19 to trainable\n",
      "Setting layer conv2d_24 to trainable\n",
      "Epoch 1/20\n",
      "4/4 [==============================] - 9s 2s/step - loss: 1.0721 - accuracy: 0.9252 - val_loss: 0.2568 - val_accuracy: 0.9259\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.2770 - accuracy: 0.9256 - val_loss: 0.2263 - val_accuracy: 0.9259\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.2002 - accuracy: 0.9256 - val_loss: 0.1668 - val_accuracy: 0.9259\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.1604 - accuracy: 0.9256 - val_loss: 0.1481 - val_accuracy: 0.9259\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.1456 - accuracy: 0.9256 - val_loss: 0.1367 - val_accuracy: 0.9260\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.1320 - accuracy: 0.9321 - val_loss: 0.1184 - val_accuracy: 0.9476\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.1173 - accuracy: 0.9497 - val_loss: 0.1114 - val_accuracy: 0.9552\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.1120 - accuracy: 0.9546 - val_loss: 0.1071 - val_accuracy: 0.9593\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.1090 - accuracy: 0.9580 - val_loss: 0.1051 - val_accuracy: 0.9578\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.1081 - accuracy: 0.9570 - val_loss: 0.0994 - val_accuracy: 0.9613\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.1048 - accuracy: 0.9585 - val_loss: 0.1050 - val_accuracy: 0.9605\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.1043 - accuracy: 0.9592 - val_loss: 0.0985 - val_accuracy: 0.9617\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.1001 - accuracy: 0.9604 - val_loss: 0.0956 - val_accuracy: 0.9626\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0971 - accuracy: 0.9611 - val_loss: 0.0934 - val_accuracy: 0.9625\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0950 - accuracy: 0.9620 - val_loss: 0.0914 - val_accuracy: 0.9635\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0948 - accuracy: 0.9625 - val_loss: 0.0893 - val_accuracy: 0.9645\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0936 - accuracy: 0.9627 - val_loss: 0.0911 - val_accuracy: 0.9646\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0927 - accuracy: 0.9634 - val_loss: 0.0876 - val_accuracy: 0.9654\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 7s 2s/step - loss: 0.0911 - accuracy: 0.9639 - val_loss: 0.0860 - val_accuracy: 0.9660\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 6s 2s/step - loss: 0.0894 - accuracy: 0.9648 - val_loss: 0.0883 - val_accuracy: 0.9655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dd729d1ba0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = classification_model()\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "model.fit(train_x2,train_y2,epochs=25)\n",
    "\n",
    "unet = unet_model_transfer(model)\n",
    "\n",
    "# first, only train last layer\n",
    "for layer in unet.layers:\n",
    "  if \"L\" in layer.name:\n",
    "    print(\"Setting layer %s to not trainable\" % layer.name)\n",
    "    layer.trainable = False\n",
    "unet.compile(loss=mycrossentropy,optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),metrics=['accuracy']) \n",
    "unet.fit(train_x, train_y, batch_size = 10, epochs = running_epochs,validation_data=(test_x, test_y))\n",
    "\n",
    "# finally train all layers\n",
    "for layer in unet.layers:\n",
    "    print(\"Setting layer %s to trainable\" % layer.name)\n",
    "    layer.trainable = True\n",
    "unet.compile(loss=mycrossentropy,optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),metrics=['accuracy']) \n",
    "unet.fit(train_x, train_y, batch_size = 10, epochs = running_epochs,validation_data=(test_x, test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
